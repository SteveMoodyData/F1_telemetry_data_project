{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Telemetry Analysis - Complete Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for F1 telemetry analysis:\n",
    "1. Data ingestion with FastF1\n",
    "2. Distributed processing with PySpark\n",
    "3. Data quality validation\n",
    "4. ML-powered predictive analytics\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from ingestion.telemetry_loader import TelemetryLoader\n",
    "from processing.spark_processor import SparkProcessor\n",
    "from quality.telemetry_validator import TelemetryQualityValidator\n",
    "from ml.lap_time_predictor import LapTimeFeatureEngineer, LapTimePredictorModel\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "Load F1 race data using FastF1 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loader\n",
    "loader = TelemetryLoader(cache_dir='../data/cache')\n",
    "\n",
    "# Load 2024 Bahrain GP Race\n",
    "print(\"Loading 2024 Bahrain GP...\")\n",
    "session = loader.load_session(2024, 'Bahrain', 'R')\n",
    "\n",
    "# Extract lap data\n",
    "laps_df = loader.extract_lap_data(session)\n",
    "print(f\"\\nLoaded {len(laps_df)} laps from {laps_df['Driver'].nunique()} drivers\")\n",
    "\n",
    "laps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA - Lap time distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(laps_df['LapTime'].dropna(), bins=50, edgecolor='black')\n",
    "plt.xlabel('Lap Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Lap Time Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "tire_counts = laps_df['Compound'].value_counts()\n",
    "plt.bar(tire_counts.index, tire_counts.values)\n",
    "plt.xlabel('Tire Compound')\n",
    "plt.ylabel('Number of Laps')\n",
    "plt.title('Laps by Tire Compound')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PySpark Processing\n",
    "\n",
    "Process data through medallion architecture layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark processor\n",
    "processor = SparkProcessor()\n",
    "\n",
    "# Process through layers\n",
    "bronze_df = processor.process_bronze_laps(laps_df)\n",
    "print(f\"Bronze layer: {bronze_df.count()} records\")\n",
    "\n",
    "silver_df = processor.process_silver_laps(bronze_df)\n",
    "print(f\"Silver layer: {silver_df.count()} records\")\n",
    "\n",
    "# Show sample of enriched data\n",
    "silver_df.select(\n",
    "    'Driver', 'LapNumber', 'LapTime', 'CompoundStd', \n",
    "    'DeltaToFastest', 'TimeMismatch'\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gold layer analytics\n",
    "gold_drivers = processor.process_gold_driver_stats(silver_df)\n",
    "gold_tires = processor.process_gold_tire_analysis(silver_df)\n",
    "\n",
    "print(\"\\n=== Driver Performance Stats ===\")\n",
    "gold_drivers.orderBy('FastestLap').show(10, truncate=False)\n",
    "\n",
    "print(\"\\n=== Tire Compound Analysis ===\")\n",
    "gold_tires.orderBy('AvgLapTime').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Validation\n",
    "\n",
    "Run comprehensive quality checks on the telemetry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to Pandas for validation\n",
    "laps_validated = silver_df.toPandas()\n",
    "\n",
    "# Run validation\n",
    "validator = TelemetryQualityValidator()\n",
    "passed, report = validator.validate_lap_data(laps_validated)\n",
    "\n",
    "print(\"=== Data Quality Report ===\")\n",
    "print(f\"Validation Status: {'PASSED' if passed else 'FAILED'}\")\n",
    "print(f\"Total Rows: {report['total_rows']}\")\n",
    "print(f\"Schema Valid: {report['schema_valid']}\")\n",
    "print(f\"\\nIssues: {len(report['issues'])}\")\n",
    "for issue in report['issues']:\n",
    "    print(f\"  - {issue}\")\n",
    "print(f\"\\nWarnings: {len(report['warnings'])}\")\n",
    "for warning in report['warnings']:\n",
    "    print(f\"  - {warning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound performance analysis\n",
    "compound_analysis = validator.validate_cross_compound_performance(laps_validated)\n",
    "\n",
    "print(\"\\n=== Cross-Compound Performance Analysis ===\")\n",
    "for gp, data in compound_analysis.items():\n",
    "    print(f\"\\n{gp}:\")\n",
    "    for compound in data['compound_performance']:\n",
    "        print(f\"  {compound['Compound']}: \"\n",
    "              f\"Median={compound['median']:.2f}s, \"\n",
    "              f\"Count={compound['count']}\")\n",
    "    if 'expected_order' in data:\n",
    "        print(f\"  Expected order: {data['expected_order']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning - Lap Time Prediction\n",
    "\n",
    "Build and train predictive models for lap time forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "engineer = LapTimeFeatureEngineer()\n",
    "df_features = engineer.engineer_lap_features(laps_validated)\n",
    "\n",
    "print(f\"Feature engineering complete: {len(df_features.columns)} total columns\")\n",
    "print(\"\\nNew features created:\")\n",
    "new_cols = set(df_features.columns) - set(laps_validated.columns)\n",
    "for col in sorted(new_cols):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y, feature_names = engineer.prepare_model_data(df_features)\n",
    "\n",
    "print(f\"Model dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Target range: {y.min():.2f}s to {y.max():.2f}s\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple model types\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for model_type in ['xgboost', 'lightgbm', 'random_forest']:\n",
    "    print(f\"\\n=== Training {model_type.upper()} ===\")\n",
    "    \n",
    "    model = LapTimePredictorModel(model_type=model_type)\n",
    "    model.train(X_train, y_train, optimize=False)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    models[model_type] = model\n",
    "    results[model_type] = metrics\n",
    "    \n",
    "    print(f\"MAE: {metrics['mae']:.3f}s\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.3f}s\")\n",
    "    print(f\"R²: {metrics['r2']:.3f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics_to_plot = ['mae', 'rmse', 'r2', 'mape']\n",
    "titles = ['Mean Absolute Error (s)', 'Root Mean Squared Error (s)', \n",
    "          'R² Score', 'Mean Absolute Percentage Error (%)']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    results_df[metric].plot(kind='bar', ax=ax, color=['#FF1E1E', '#00D2BE', '#FFF500'])\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Rotate labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best model\n",
    "best_model_name = results_df['r2'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name.upper()}\")\n",
    "print(\"\\n=== Top 15 Most Important Features ===\")\n",
    "print(best_model.feature_importance.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = best_model.feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top 15 Feature Importances - {best_model_name.upper()}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction analysis\n",
    "y_pred = best_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Lap Time (s)')\n",
    "axes[0].set_ylabel('Predicted Lap Time (s)')\n",
    "axes[0].set_title('Actual vs Predicted Lap Times')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Lap Time (s)')\n",
    "axes[1].set_ylabel('Residual (s)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"Mean: {residuals.mean():.4f}s\")\n",
    "print(f\"Std Dev: {residuals.std():.4f}s\")\n",
    "print(f\"95% within: ±{1.96 * residuals.std():.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "model_path = f'../data/models/{best_model_name}_lap_predictor.pkl'\n",
    "best_model.save_model(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save results summary\n",
    "results_df.to_csv('../data/output/model_comparison.csv')\n",
    "print(\"Results saved to ../data/output/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "processor.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Expand Dataset**: Load multiple races/seasons for more robust training\n",
    "2. **Hyperparameter Tuning**: Run Optuna optimization for better performance\n",
    "3. **Advanced Features**: \n",
    "   - Weather data integration\n",
    "   - Detailed telemetry features (throttle, brake, speed profiles)\n",
    "   - Driver-specific learned patterns\n",
    "4. **Production Pipeline**:\n",
    "   - Delta Lake storage\n",
    "   - Scheduled data refreshes\n",
    "   - Model retraining automation\n",
    "5. **Deployment**:\n",
    "   - Real-time prediction API\n",
    "   - Interactive dashboard\n",
    "   - Race strategy optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
